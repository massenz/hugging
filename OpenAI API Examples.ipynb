{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbfe638e-9be2-4026-88b9-d2dd09039792",
   "metadata": {},
   "source": [
    "# OpenAI Thread API\n",
    "\n",
    "## Overview\n",
    "\n",
    "This is a brief end-to-end example of how to use the more recent Assistants/Threads OpenAI API; the [docs]() \n",
    "are reasonably good, but only provide piecemeal examples of how to use the Python API, and I wanted to\n",
    "have a feel for the full end-to-end functionality.\n",
    "\n",
    "Here, we will create one `Assistant`, then a `Thread` (OpenAI's evolution of the `Chat Completions`) and then\n",
    "execute a `Run` of the `Thread` using the given `Assistant`, to provide the LLM output.\n",
    "\n",
    "Obviously, in a \"real\" application, this would be execute in an actual conversation thread, iteratively sending\n",
    "to the LLM subsequent user prompts, until the problem is solved.\n",
    "\n",
    "The full code is in [this GitHub repository](https://github.com/massenz/hugging) along with other code samples\n",
    "using the [Hugging Face](https://hugging.dev) API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4030881a-18f5-4f74-b5b0-4dbe3854de49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import read_env\n",
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "# These are only needed for type annotations\n",
    "from openai.resources.beta.assistants import Assistant\n",
    "from openai.types.beta.thread import Thread\n",
    "from openai.types.beta.threads.run import Run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf93f99-c5a8-48c2-99aa-a1a338520e5a",
   "metadata": {},
   "source": [
    "### Authentication\n",
    "\n",
    "Recently, OpenAI has introduced the concept of a `Project` (make sure you are using\n",
    "the most recent version of the `openai` package, `1.25.1`):\n",
    "\n",
    "```\n",
    "└─( pip freeze | grep -i openai\n",
    "openai==1.25.1\n",
    "```\n",
    "Assistants are scoped by-project, so this is important; omitting the `project` keyword arg\n",
    "will make use of the `default project` (whatever was in the org account prior to the\n",
    "projects' introduction).\n",
    "\n",
    "More info [here](https://platform.openai.com/docs/api-reference/authentication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c4143b5a-9f53-45db-b5e7-fdf544dde7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = read_env()\n",
    "client = OpenAI(api_key=env['oai_token'], \n",
    "                organization=env['oai_org_id'], \n",
    "                project=env['oai_project_id'],\n",
    "         )\n",
    "\n",
    "OAI_MODEL = env.get('oai_model') or 'gpt-4-turbo'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d171f37-9d7e-4481-8cf9-988c9393ada5",
   "metadata": {},
   "source": [
    "# Assistants\n",
    "\n",
    "An `Assistant` receives specific instructions, a personality, a specialization and is used to customize the responses from the LLM (what is usually called `Prompt Engineering`).\n",
    "\n",
    "In the example below, we create a `Go Developer` assistant, then create two utility functions to retrieve all the assistants in the Project, and to reverse-map their names to their `id`.\n",
    "\n",
    "**Note**\n",
    "> We have \"pinned\" the model to the `OAI_MODEL` value above, but this can be dynamically changed.\n",
    "> Also, make sure that you use at least `gpt-4-turbo` which is up-to-date to Dec 2023\n",
    "> (more info [here](https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "27da6bf1-0ac0-4b19-89f8-59302d47c84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_assistants() -> dict[str, str]:\n",
    "    \"\"\" Retrieves the list of assistants, scoped by org/project.\"\"\"\n",
    "    my_assistants = client.beta.assistants.list(\n",
    "        order=\"desc\",\n",
    "        limit=\"20\",\n",
    "    )\n",
    "    assts = {}\n",
    "    for a in my_assistants.data:\n",
    "        assts[a.name] = a.id\n",
    "    return assts\n",
    "\n",
    "\n",
    "def get_asst_id(name: str) -> str:\n",
    "    \"\"\"Given an Assistant's name, will return its ID.\"\"\"\n",
    "    return get_assistants().get(name, None)\n",
    "\n",
    "\n",
    "def get_asst_by_name(name: str) -> Assistant:\n",
    "    \"\"\"Retrieves an Assistant by name\"\"\"\n",
    "    aid = get_asst_id(name)\n",
    "    return client.beta.assistants.retrieve(assistant_id=aid)\n",
    "\n",
    "\n",
    "def new_assistant(name: str, instructions: str) -> Assistant:\n",
    "    # First, check that an assistant with that name\n",
    "    # does not already exist; OpenAI will otherwise create a\n",
    "    # new one with the same name, which is probably not what the \n",
    "    # user expected.\n",
    "    aid = get_asst_id(name)\n",
    "    if aid is not None:\n",
    "        print(f\"WARN: Assistant {name} already exists, updating instructions.\")\n",
    "        return client.beta.assistants.update(\n",
    "            assistant_id=aid,\n",
    "            name=name,\n",
    "            instructions=instructions,\n",
    "            model=OAI_MODEL,\n",
    "        )\n",
    "    return client.beta.assistants.create(\n",
    "                name=name,\n",
    "                instructions=instructions,\n",
    "                tools=[{\"type\": \"code_interpreter\"}],\n",
    "                model=OAI_MODEL,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0b2c54-d5cf-4b02-bd12-f2242b93aa9e",
   "metadata": {},
   "source": [
    "# Threads\n",
    "\n",
    "A `Thread` is exactly what the name says: a conversation thread, with a sequence of messages, between the `user` and the `assistant`; it is used in a `Run` (see below) to drive the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "01144940-8ce9-4e5a-a075-8699e56f1df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_thread() -> Thread:\n",
    "    return client.beta.threads.create()\n",
    "    \n",
    "def add_msg_to_thread(thread: Thread, message: str) -> None:\n",
    "    message = client.beta.threads.messages.create(\n",
    "        thread_id=thread.id,\n",
    "        role=\"user\",\n",
    "        content=content,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793e7193-acbc-44e7-880c-9bffad0422f3",
   "metadata": {},
   "source": [
    "# Runs\n",
    "\n",
    "A `Run` embodies the execution of a `Thread` for an `Assistant`; see [here](https://platform.openai.com/docs/assistants/how-it-works/run-lifecycle) for a full description of a `Run`'s lifecycle.\n",
    "\n",
    "Once the `Run` is in the `\"completed\"` state...\n",
    "> You can also continue the conversation by adding more user Messages to the Thread and creating another Run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "14774636-2876-4409-8511-d1c6486f9c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_run(thread: Thread, asst_name: str) -> Run:\n",
    "    return client.beta.threads.runs.create(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=get_asst_id(asst_name),\n",
    "    )\n",
    "\n",
    "def wait_on_run(run, thread, timeout: int = 120, interval: float = 2) -> bool:\n",
    "    count = 0\n",
    "    retries = timeout / interval\n",
    "    while run.status == \"queued\" or run.status == \"in_progress\":\n",
    "        run = client.beta.threads.runs.retrieve(\n",
    "            thread_id=thread.id,\n",
    "            run_id=run.id,\n",
    "        )    \n",
    "        count += 1\n",
    "        if count > retries:\n",
    "            # We have exceeded the timeout, we need to first \n",
    "            # cancel the run to avoid incurring further costs.\n",
    "            client.beta.threads.runs.cancel(\n",
    "                thread_id=thread.id,\n",
    "                run_id=run.id,\n",
    "            )\n",
    "            return False\n",
    "        time.sleep(0.5)\n",
    "    return run.status == \"completed\"\n",
    "\n",
    "def get_response(thread: Thread) -> str:\n",
    "    \"\"\"Simplified approach to retrieve just the latest message from a 'completed' run.\n",
    "\n",
    "    Generally, there may be several assistant's messages in a Run; however, in the simple\n",
    "    case here, we can assume the LLM provided its response in a single message, the last\n",
    "    in the queue.\n",
    "    \"\"\"\n",
    "    messages = client.beta.threads.messages.list(\n",
    "        thread_id=thread.id,\n",
    "        order=\"asc\",\n",
    "    )\n",
    "    if len(messages.data) == 0:\n",
    "        raise ValueError(\"No messages in Thread\")\n",
    "    return messages.data[-1].content[0].text.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9604f55-0b98-4b40-ad54-ce9ba7460950",
   "metadata": {},
   "source": [
    "# Using the LLM\n",
    "\n",
    "We are now ready to request our `Assistant` to provide us with guidance and help; this is typically done in a loop, here we just show a couple of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7a5523c3-4bdd-4a66-9966-cb377c623113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: Assistant Go Dev already exists, updating instructions.\n",
      "Go Dev says:\n",
      "Here's how you can use the `go-openai` package in Go to interact with the OpenAI API, particularly focusing on creating an assistant, managing a thread, and processing messages with OpenAI GPT-4.\n",
      "\n",
      "### Setup\n",
      "\n",
      "1. **Create an Assistant**: An assistant needs to be created, which can be attached to subsequent threads for message handling.\n",
      "2. **Manage a Thread**: A thread is like a conversation or dialog which holds the messages.\n",
      "3. **Execute the Run**: This actually sends the message to OpenAI and gets the response which is processed accordingly.\n",
      "\n",
      "First, please ensure that you have the `go-openai` library installed:\n",
      "\n",
      "```bash\n",
      "go get github.com/sashabaranov/go-openai\n",
      "```\n",
      "\n",
      "Here's an example illustrating the entire process:\n",
      "\n",
      "```go\n",
      "package main\n",
      "\n",
      "import (\n",
      "\t\"context\"\n",
      "\t\"fmt\"\n",
      "\t\"github.com/sashabaranov/go-openai/openai\"\n",
      ")\n",
      "\n",
      "// Initializes a new OpenAI client with your API key\n",
      "func initClient(apiKey string) *openai.Client {\n",
      "\tclient := openai.NewClient(apiKey)\n",
      "\treturn client\n",
      "}\n",
      "\n",
      "// createAssistant creates a new Assistant and returns its ID\n",
      "func createAssistant(client *openai.Client, context context.Context) (string, error) {\n",
      "\tassistant, err := client.Assistant.Create(context, openai.AssistantCreateParams{\n",
      "\t\tName:        \"My GPT-4 Assistant\",\n",
      "\t\tDescription: \"A GPT-4 based assistant\",\n",
      "\t\tModel:       \"gpt-4\", // Assuming the 'gpt-4' exists and is available by the time of the implementation\n",
      "\t})\n",
      "\tif err != nil {\n",
      "\t\treturn \"\", err\n",
      "\t}\n",
      "\treturn assistant.ID, nil\n",
      "}\n",
      "\n",
      "// sendMessage creates a message in the specified thread\n",
      "func sendMessage(client *openai.Client, ctx context.Context, threadID, content string) error {\n",
      "\t_, err := client.Message.Create(ctx, threadID, openai.MessageCreateParams{\n",
      "\t\tContent: content,\n",
      "\t})\n",
      "\treturn err\n",
      "}\n",
      "\n",
      "// runExecution executes the thread and returns the response\n",
      "func runExecution(client *openai.Client, ctx context.Context, threadID string) (string, error) {\n",
      "\trun, err := client.Thread.ExecuteRun(ctx, threadID)\n",
      "\tif err != nil {\n",
      "\t\treturn \"\", err\n",
      "\t}\n",
      "\treturn run.Data.Output, nil\n",
      "}\n",
      "\n",
      "func main() {\n",
      "\tapiKey := \"YOUR_OPENAI_API_KEY\"\n",
      "\tctx := context.Background()\n",
      "\tclient := initClient(apiKey)\n",
      "\n",
      "\t// Create an assistant\n",
      "\tassistantID, err := createAssistant(client, ctx)\n",
      "\tif err != nil {\n",
      "\t\tfmt.Println(\"Error creating assistant:\", err)\n",
      "\t\treturn\n",
      "\t}\n",
      "\n",
      "\t// Create a thread linked to the assistant\n",
      "\tthreadID := \"some-thread-id\" // You would create or manage thread ID in practice\n",
      "\n",
      "\t// Send a message\n",
      "\terr = sendMessage(client, ctx, threadID, \"Hello, how can I use GPT-4 to generate text?\")\n",
      "\tif err != nil {\n",
      "\t\tfmt.Println(\"Error sending message:\", err)\n",
      "\t\treturn\n",
      "\t}\n",
      "\n",
      "\t// Execute run and get the response\n",
      "\tresponse, err := runExecution(client, ctx, threadID)\n",
      "\tif err != nil {\n",
      "\t\tfmt.Println(\"Error executing run for thread:\", err)\n",
      "\t\treturn\n",
      "\t}\n",
      "\n",
      "\tfmt.Println(\"Response from OpenAI GPT-4:\", response)\n",
      "}\n",
      "```\n",
      "\n",
      "### Notes\n",
      "- Ensure your OpenAI API key is correctly specified.\n",
      "- Replace `\"YOUR_OPENAI_API_KEY\"` with your actual API key.\n",
      "- Make sure that `threadID` is correctly managed as per your application's needs (the provided example assumes a dummy `threadID`).\n",
      "- Adjust the model as needed, and make sure the corresponding model identifiers are correct and available for use at the time of your implementation.\n",
      "\n",
      "This example should provide a complete workflow from creating an assistant to obtaining responses from OpenAI GPT-4.\n"
     ]
    }
   ],
   "source": [
    "name = \"Go Dev\"\n",
    "\n",
    "instructions = \"\"\"You are a dedicated GoLang developer,\n",
    "and have detailed knowledge of the Go OpenAI library\n",
    "called github.com/sashabaranov/go-openai.\n",
    "You will provide your answers formatted in Markdown,\n",
    "with appropriate code examples.\n",
    "The code should be complete and ready to be compiled: do not\n",
    "provide just fragments, but the full code for functions and types.\n",
    "\"\"\"\n",
    "\n",
    "content = \"\"\"\n",
    "Using the `go-openai` library in Go, please show me a function \n",
    "that creates an OpenAI Assistant, and associate it to\n",
    "a Thread;\n",
    "then create a function that takes that Thread, and an arbitrary\n",
    "user content, and creates a message and adds it to the Thread;\n",
    "finally, a third function should execute the Run with the Thread,\n",
    "and return a string with the response from OpenAI GPT-4.\n",
    "\n",
    "Please, do not make things up; if you don't know how to do something,\n",
    "please feel free to ask the question, and we'll take it from there.\n",
    "\"\"\"\n",
    "\n",
    "# 0. We need an assistant\n",
    "new_assistant(name, instructions)\n",
    "\n",
    "# 1. Get a Thread, and append a message to it\n",
    "thread = new_thread()\n",
    "add_msg_to_thread(thread, content)\n",
    "\n",
    "# 2. Get a new Run, and associate it with our Thread\n",
    "#    We will use the Assistant (Go Dev) we created before.\n",
    "run = new_run(thread=thread, asst_name=name)\n",
    "\n",
    "# 3. We then ask GPT for advice\n",
    "if wait_on_run(run, thread):\n",
    "    response = get_response(thread)\n",
    "    print(f\"{name} says:\\n{response}\")\n",
    "else:\n",
    "    print(f\"We failed! Status: {run.status}, {run.incomplete_details}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4db8bbb2-9a86-4672-919c-5e1eda89dafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go Dev says:\n",
      "Certainly! I'll provide the function `GetConfig` which will read the configuration details from a `config.yaml` file using the popular `gopkg.in/yaml.v2` package for YAML parsing in Go. Below is the full code including adjustments to read the config file and retrieve necessary settings.\n",
      "\n",
      "Firstly, you will need to install the YAML package:\n",
      "\n",
      "```bash\n",
      "go get gopkg.in/yaml.v2\n",
      "```\n",
      "\n",
      "### Full Code Revised with Configuration Reader\n",
      "\n",
      "```go\n",
      "package main\n",
      "\n",
      "import (\n",
      "\t\"context\"\n",
      "\t\"fmt\"\n",
      "\t\"github.com/sashabaranov/go-openai/openai\"\n",
      "\t\"gopkg.in/yaml.v2\"\n",
      "\t\"io/ioutil\"\n",
      "\t\"log\"\n",
      ")\n",
      "\n",
      "type Config struct {\n",
      "\tOpenAI struct {\n",
      "\t\tAPIKey string `yaml:\"api_key\"`\n",
      "\t\tOrgID  string `yaml:\"org_id\"`\n",
      "\t\tPrjID  string `yaml:\"prj_id\"`\n",
      "\t} `yaml:\"openai\"`\n",
      "\tModel string `yaml:\"model\"`\n",
      "}\n",
      "\n",
      "// GetConfig reads from a YAML file and unmarshals into Config struct\n",
      "func GetConfig(filename string) (Config, error) {\n",
      "\tvar config Config\n",
      "\n",
      "\tdata, err := ioutil.ReadFile(filename)\n",
      "\tif err != nil {\n",
      "\t\treturn Config{}, err\n",
      "\t}\n",
      "\n",
      "\terr = yaml.Unmarshal(data, &config)\n",
      "\tif err != nil {\n",
      "\t\treturn Config{}, err\n",
      "\t}\n",
      "\n",
      "\treturn config, nil\n",
      "}\n",
      "\n",
      "// Initializes a new OpenAI client with your API key\n",
      "func initClient(apiKey, orgID, prjID string) *openai.Client {\n",
      "\tclient := openai.NewClient(apiKey).WithOrganization(orgID).WithProject(prjID)\n",
      "\treturn client\n",
      "}\n",
      "\n",
      "// Functions createAssistant, sendMessage and runExecution are assumed to be the same\n",
      "\n",
      "func main() {\n",
      "\t// Load configuration\n",
      "\tconfig, err := GetConfig(\"config.yaml\")\n",
      "\tif err != nil {\n",
      "\t\tlog.Fatalf(\"Error loading configuration: %v\", err)\n",
      "\t}\n",
      "\n",
      "\tctx := context.Background()\n",
      "\tclient := initClient(config.OpenAI.APIKey, config.OpenAI.OrgID, config.OpenAI.PrjID)\n",
      "\n",
      "\t// Create an assistant\n",
      "\tassistantID, err := createAssistant(client, ctx)\n",
      "\tif err != nil {\n",
      "\t\tfmt.Println(\"Error creating assistant:\", err)\n",
      "\t\treturn\n",
      "\t}\n",
      "\n",
      "\t// Create a thread linked to the assistant\n",
      "\tthreadID := \"some-thread-id\" // You would create or manage thread ID in practice\n",
      "\n",
      "\t// Send a message\n",
      "\terr = sendMessage(client, ctx, threadID, \"Hello, how can I use GPT-4 to generate text?\")\n",
      "\tif err != nil {\n",
      "\t\tfmt.Println(\"Error sending message:\", err)\n",
      "\t\treturn\n",
      "\t}\n",
      "\n",
      "\t// Execute run and get the response\n",
      "\tresponse, err := runExecution(client, ctx, threadID)\n",
      "\tif err != nil {\n",
      "\t\tfmt.Println(\"Error executing run for thread:\", err)\n",
      "\t\treturn\n",
      "\t}\n",
      "\n",
      "\tfmt.Println(\"Response from OpenAI GPT-4:\", response)\n",
      "}\n",
      "```\n",
      "\n",
      "### Explanation\n",
      "- The `Config` struct is defined with nested structs to map it to the structure of `config.yaml`.\n",
      "- The `GetConfig` function reads the file, unmarshals it into a `Config` struct, and returns it.\n",
      "- `initClient` now also receives `orgID` and `prjID` which are used to set additional configurations for the client.\n",
      "- The `main` function reads configurations from `config.yaml` before initializing and using the OpenAI client.\n",
      "\n",
      "This maintains organization and project scoped configurations, which is ideal for practical applications especially when dealing with multiple configurations across environments or projects.\n"
     ]
    }
   ],
   "source": [
    "# We can append another message to the Thread and continue the conversation.\n",
    "content = \"\"\"\n",
    "We should be able to read the API_KEY and an ORG_ID and PRJ_ID\n",
    "from a configuration YAML file (for now, assume it is called `config.yaml`)\n",
    "with the following structure:\n",
    "\n",
    "```yaml\n",
    "openai:\n",
    "    api_key: ts_12345\n",
    "    org_id: org_998743424\n",
    "    prj_id: prj_afad4456\n",
    "\n",
    "model: gpt-4-turbo\n",
    "```\n",
    "\n",
    "Please add a `GetConfig(name string) (Config, error)` function that does that\n",
    "and invoke it from the `main()` function.\n",
    "\"\"\"\n",
    "\n",
    "add_msg_to_thread(thread, content)\n",
    "run = new_run(thread=thread, asst_name=name)\n",
    "if wait_on_run(run, thread):\n",
    "    response = get_response(thread)\n",
    "    print(f\"{name} says:\\n{response}\")\n",
    "else:\n",
    "    print(f\"We failed! Status: {run.status}, {run.incomplete_details}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cdba88-e958-4245-80f5-46ec38485944",
   "metadata": {},
   "source": [
    "### Put all the pieces together\n",
    "\n",
    "Hopefully, by now it should be pretty obvious how all the moving parts can be put together (ideally, in appropriate classes) and the whole process ran in a loop, until the user declares the task complete.\n",
    "\n",
    "The Jupyter Notebook can be found [here](https://github.com/massenz/hugging/blob/main/OpenAI%20API%20Examples.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0a84c8-7b42-4e21-b707-abee597dff17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
